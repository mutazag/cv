{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596514701601",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.10.0\n"
    }
   ],
   "source": [
    "import json\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore, Dataset, Environment, Experiment\n",
    "from azureml.data import FileDataset\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PipelineParameter\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "version = dict(zip(['major','minor','patch'], azureml.core.VERSION.split('.')))\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if int(version['major']) >= 1: \n",
    "    if int(version['minor']) == 10:\n",
    "        from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep        \n",
    "    else:\n",
    "        from azureml.contrib.pipeline.steps import ParallelRunConfig, ParallelRunStep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found compute target: aml-compute1\n"
    }
   ],
   "source": [
    "compute_name = \"aml-compute1\"\n",
    "vm_size = \"STANDARD_DS1_v2\"\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target: ' + compute_name)\n",
    "else:\n",
    "    print('compute target not found, refer to 02_create_compute_cluster to create compute target...')"
   ]
  },
  {
   "source": [
    "datastore_name = 'godzilla'\n",
    "if datastore_name in ws.datastores:\n",
    "    datastore = ws.datastores[datastore_name]\n",
    "    if datastore and type(datastore) is Datastore: \n",
    "        print('Found datastore: ' + datastore_name)\n",
    "else: \n",
    "    print('datastore not found...')\n",
    "\n",
    "images_dataset_name = 'images_partition'\n",
    "datapath_on_datastore = DataPath(datastore=datastore, path_on_datastore='images')\n",
    "# path_on_datastore = datastore.path('images')\n",
    "# input_images_dataset = Dataset.File.from_files(path=path_on_datastore, validate=False)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "azureml.data.datapath.DataPath"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "type(datapath_on_datastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = PipelineData(name=\"scores\", \n",
    "                          datastore=ws.get_default_datastore(), \n",
    "                          output_path_on_compute=\"batchscoring/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "$AZUREML_DATAREFERENCE_scores"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "source": [
    "# pipeline parameters and datapath\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kv = ws.get_default_keyvault()\n",
    "# print(len(kv.get_secret(pipeline_kv_readapi)))\n",
    "pipeline_inpart = PipelineParameter(name=\"pipeline_inpart\", default_value='2020/07/28')\n",
    "pipeline_kv_customimg = PipelineParameter(name=\"pipeline_kv_customimg\", default_value='api-custom-vision')\n",
    "pipeline_kv_readapi = PipelineParameter(name=\"pipeline_kv_readapi\", default_value='api-readapi')\n",
    "pipeline_datapath_param = PipelineParameter(name='pipeline_datapath', default_value=datapath_on_datastore)\n",
    "datapath_input = (pipeline_datapath_param, DataPathComputeBinding(mode='mount'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PipelineParameter_Name:pipeline_inpart_Default:2020/07/28\nPipelineParameter_Name:pipeline_datapath_Default:<azureml.data.datapath.DataPath object at 0x000001BB5F0E36D0>\n(<azureml.pipeline.core.graph.PipelineParameter object at 0x000001BB5F585FD0>, <azureml.data.datapath.DataPathComputeBinding object at 0x000001BB5F585310>)\n"
    }
   ],
   "source": [
    "print(pipeline_inpart)\n",
    "print(pipeline_datapath_param)\n",
    "print(datapath_input)"
   ]
  },
  {
   "source": [
    "# python envinronment configuration "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# conda dependencies \n",
    "env_name = 'MAG-ParallelRunEnv'\n",
    "print(env_name in ws.environments)\n",
    "if env_name in ws.environments: \n",
    "    env = ws.environments.get(env_name)\n",
    "    if env and type(env) is Environment: \n",
    "        print('Found environment: ' + env_name)\n",
    "else: \n",
    "    print('environment not found, refer to 01_config_notebook to register envinroment...')\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 24,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\nFound environment: MAG-ParallelRunEnv\n"
    }
   ]
  },
  {
   "source": [
    "# parallel run config and step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "parallelrunconfig = ParallelRunConfig(\n",
    "    environment=env, \n",
    "    entry_script='minibatch_process.py', \n",
    "    error_threshold=1,\n",
    "    output_action='append_row', \n",
    "    compute_target=compute_target, \n",
    "    node_count=1, \n",
    "    process_count_per_node=2,\n",
    "    mini_batch_size='2',\n",
    "    source_directory='scripts', \n",
    "    description='description of batch step config',\n",
    "    logging_level='INFO'\n",
    ")\n",
    "\n",
    "\n",
    "#parallelrunconfig('script', 'other stuff')\n",
    "parallelrunstep = ParallelRunStep(\n",
    "    name='cv-detection-batch-dataset-step', \n",
    "    parallel_run_config=parallelrunconfig, \n",
    "    inputs=[datapath_input],\n",
    "    # inputs=[DatasetConsumptionConfig('dataset_param_config', pipeline_dataset_param).as_mount()], \n",
    "    # inputs=[ Dataset.File.from_files((godzilla_datastore, 'images')).as_named_input('anpr_images').as_mount()],\n",
    "    # inputs=[]\n",
    "    side_inputs=[], \n",
    "    output=output_dir,\n",
    "    arguments=['--input_partition', pipeline_inpart, '--kv_customimage', pipeline_kv_customimg, '--kv_readapi',)pipeline_kv_readapi], \n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "\n",
    "# ('config', 'inputs as mount', 'arguments passing in pipeline args')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "Step input must be of any type: dict_keys([<class 'azureml.data.tabular_dataset.TabularDataset'>, <class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset'>, <class 'azureml.data.file_dataset.FileDataset'>, <class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset'>]), found <class 'tuple'>",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e3022c78e0f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m#parallelrunconfig('script', 'other stuff')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m parallelrunstep = ParallelRunStep(\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cv-detection-batch-dataset-step'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mparallel_run_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallelrunconfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python383\\lib\\site-packages\\azureml\\pipeline\\steps\\parallel_run_step.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, parallel_run_config, inputs, output, side_inputs, arguments, allow_reuse)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mallow_reuse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \"\"\"\n\u001b[1;32m--> 123\u001b[1;33m         super(ParallelRunStep, self).__init__(\n\u001b[0m\u001b[0;32m    124\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mparallel_run_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparallel_run_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python383\\lib\\site-packages\\azureml\\pipeline\\core\\_parallel_run_step_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, parallel_run_config, inputs, output, side_inputs, arguments, allow_reuse)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module_logger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_pystep_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python383\\lib\\site-packages\\azureml\\pipeline\\core\\_parallel_run_step_base.py\u001b[0m in \u001b[0;36m_validate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_parallel_run_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python383\\lib\\site-packages\\azureml\\pipeline\\core\\_parallel_run_step_base.py\u001b[0m in \u001b[0;36m_validate_inputs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;34m\"The parameter 'inputs' must be a list and have at least one element.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_ds_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_input_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minput_ds\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_ds_type\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_input_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python383\\lib\\site-packages\\azureml\\pipeline\\core\\_parallel_run_step_base.py\u001b[0m in \u001b[0;36m_get_input_type\u001b[1;34m(self, in_ds)\u001b[0m\n\u001b[0;32m    320\u001b[0m             \u001b[0mds_mapping_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mINPUT_TYPE_DICT\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m             raise Exception(\n\u001b[0m\u001b[0;32m    323\u001b[0m                 \u001b[1;34m\"Step input must be of any type: {}, found {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINPUT_TYPE_DICT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             )\n",
      "\u001b[1;31mException\u001b[0m: Step input must be of any type: dict_keys([<class 'azureml.data.tabular_dataset.TabularDataset'>, <class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset'>, <class 'azureml.data.file_dataset.FileDataset'>, <class 'azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset'>]), found <class 'tuple'>"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-06adc92525fb>, line 3)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-06adc92525fb>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    to move score data from out temp storage to blog storage\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# data transfer step \n",
    "\n",
    "to move score data from out temp storage to blog storage"
   ]
  },
  {
   "source": [
    "# prepare pipeline "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'parallelrunstep' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d23a879574e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparallelrunstep\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpipeline_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MAG-batch-paramdatapath'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpipeline_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parallelrunstep' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[parallelrunstep])\n",
    "pipeline_run = Experiment(ws, 'MAG-batch-paramdatapath').submit(pipeline)\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pipeline_run' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b9b976154b29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m published_pipeline = pipeline_run.publish_pipeline(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MAG-batchscore-datapath'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'published pipeline with datapath param 15:30pm'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mversion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1.0'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     continue_on_step_failure=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline_run' is not defined"
     ]
    }
   ],
   "source": [
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name='MAG-batchscore-datapath', \n",
    "    description='published pipeline with datapath param 15:30pm', \n",
    "    version='1.0', \n",
    "    continue_on_step_failure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'published_pipeline' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3c8fba49cd47>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpublished_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'published_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "published_pipeline.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}